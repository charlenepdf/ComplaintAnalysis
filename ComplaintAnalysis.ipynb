{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e754cdd0",
   "metadata": {},
   "source": [
    "### Install and load necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec22c836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a261347f",
   "metadata": {},
   "source": [
    "### Download necessary NLTK data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cf5f5766",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\charl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\charl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\charl\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aa2925",
   "metadata": {},
   "source": [
    "### Load dataset then combine 'Issue' and 'Sub-Issue' columns into a single text field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36ce12f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints_df = pd.read_csv(\"C:/Users/charl/Documents/Complaint Analysis/ComplaintAnalysis-clean/rows.csv.zip\",\n",
    "                           low_memory=False)\n",
    "\n",
    "complaints_df['combined_text'] = complaints_df[['Issue', 'Sub-issue']].astype(str).agg(' '.join, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d579b7c",
   "metadata": {},
   "source": [
    "### Handle NaN value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695360e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75c1f15c",
   "metadata": {},
   "source": [
    "### Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ff03c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
    "    return text\n",
    "\n",
    "complaints_df['cleaned_text'] = complaints_df['combined_text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717c3434",
   "metadata": {},
   "source": [
    "### Synonym expansion using WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb3e87d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wn.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name().lower())\n",
    "    return synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b776a63",
   "metadata": {},
   "source": [
    "### Define topic mapping using synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3e72931c",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_mapping = {\n",
    "    'debt collection': ['debt collection', 'other debt', 'medical debt'],\n",
    "    'credit reporting': ['credit reporting', 'credit repair services', 'personal consumer reports'],\n",
    "    'communication tactics': ['communication tactics', 'talked to a third-party about your debt', 'used obscene, profane, or other abusive language', 'frequent or repeated calls'],\n",
    "    'incorrect information': ['incorrect information', 'incorrect information on your report', 'information belongs to someone else'],\n",
    "    'legal action': ['threatened to take negative or legal action', 'threatened to sue you for very old debt'],\n",
    "    'notification issues': ['written notification about debt', 'didn\\'t receive enough information to verify debt', 'didn\\'t receive notice of right to dispute']\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1badda83",
   "metadata": {},
   "source": [
    "### Replace related words with topic keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d0c4a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_topic_keywords(text, topic_mapping):\n",
    "    for topic, related_words in topic_mapping.items():\n",
    "        for word in related_words:\n",
    "            text = re.sub(r'\\b' + re.escape(word) + r'\\b', topic, text)\n",
    "    return text\n",
    "\n",
    "complaints_df['mapped_text'] = complaints_df['cleaned_text'].apply(lambda x: replace_with_topic_keywords(x, topic_mapping))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fa9813",
   "metadata": {},
   "source": [
    "### Tokenize and remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df147470",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "complaints_df['tokens'] = complaints_df['mapped_text'].apply(word_tokenize)\n",
    "complaints_df['filtered_tokens'] = complaints_df['tokens'].apply(lambda x: [word for word in x if word not in stop_words])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
